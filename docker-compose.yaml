version: '3.8'

networks:
  kafka-network:
    driver: bridge


x-airflow-common:
  &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.2}
  # build: .
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASS}@airflow_postgres/${AIRFLOW_DB_NAME}
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASS}@airflow_postgres/${AIRFLOW_DB_NAME}
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    # yamllint disable rule:line-length
    # Use simple http server on scheduler for health checks
    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
    # for other purpose (development, test and especially production usage) build/extend Airflow image.
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    # The following line can be used to set a custom config file, stored in the local config folder
    # If you want to use it, outcomment it and replace airflow.cfg with the name of your config file
    # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
  volumes:
    - ./airflow-dags:/opt/airflow/dags
    - airflow-logs:/opt/airflow/logs
    - ./airflow-config.cfg:/opt/airflow/config
    - airflow-plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    airflow_postgres:
      condition: service_healthy
  networks:
    - kafka-network


services:

  # Responsible for coordination and synchronization between distributed systems, such as Kafka and HBase.
  zookeeper:
    image: ubuntu/zookeeper:3.8-22.04_edge
    container_name: zookeeper
    networks:
      - kafka-network
  # Provides a web-based user interface for monitoring and managing Zookeeper clusters.
  zoonavigator:
    image: elkozmon/zoonavigator:latest
    container_name: zoonavigator
    networks:
      - kafka-network
    ports:
      - "${ZOONAVIGATOR_UI_PORT}:9000"
    environment:
      HTTP_PORT: 9000
      CONNECTION_LOCALZK_NAME: "Local ZooKeeper"
      CONNECTION_LOCALZK_CONN: "zookeeper:2181"
    depends_on:
      - zookeeper

 # A distributed streaming platform used for real-time data ingestion and processing.
  kafka:
    image: ubuntu/kafka:3.6-22.04_edge
    container_name: kafka
    networks:
      - kafka-network
    ports:
      - "${KAFKA_PORT}:9092"
    volumes:
      - ./kafka-server.properties:/etc/kafka/server.properties
    depends_on:
      - zookeeper

  # Offers a user interface for monitoring Kafka clusters and topics.
  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    networks:
      - kafka-network
    ports:
      - "${KAFKA_UI_PORT}:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: default
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      - kafka

  # Offers real time stream processing, as opposed to spark which is used for batch processing
  flink-jobmanager:
    build:
      context: .
      dockerfile: flinkDockerfile
    container_name: flink-jobmanager
    networks:
      - kafka-network
    ports:
      - "${FLINK_UI_PORT}:8081"
      - "${FLINK_RPC_PORT_JOBMANAGER}:6123"
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager

  flink-taskmanager:
    build:
      context: .
      dockerfile: flinkDockerfile
    container_name: flink-taskmanager
    networks:
      - kafka-network
    command: taskmanager
    ports:
      - "${FLINK_RPC_PORT_TASKMANAGER}:6122"
      - "${FLINK_DATA_PORT_TASKMANAGER}:6121"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    depends_on:
      - flink-jobmanager

  # Acts as the Spark master node, managing the allocation of resources and scheduling tasks across the Spark cluster.

  spark-master:
    image: bitnami/spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    networks:
      - kafka-network
    ports:
      - "${SPARK_UI_PORT}:8080"  # Spark UI
      - "${SPARK_MASTER_PORT}:7077"  # Spark Master
    volumes:
      - spark-logs:/opt/spark/logs

  spark-worker:
    image: bitnami/spark
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    networks:
      - kafka-network
    depends_on:
      - spark-master
    volumes:
      - spark-logs:/opt/spark/logs

  # A distributed NoSQL database that provides real-time read/write access to large datasets, often used for random, real-time access to big data.
  hbase:
    image: hyness/hbase-rest-standalone
    container_name: hbase
    networks:
      - kafka-network
    volumes:
      - ./hbase-site.xml:/opt/hbase/conf/hbase-site.xml  # Mount custom configuration directory
      - hbase_data:/data/hbase
    ports:
     # - "2181:2181" #Zookeeper port, no need for it as we connect hbase to an external zookeeper
      - "${HBASE_REST_API_PORT}:8080" # REST api port, can interact with hbase api through here
      - "${HBASE_MASTER_PORT}:16000" # hbase master port
      - "${HBASE_UI_PORT}:16010" # UI port
      - "${HBASE_REGIONSERVER_PORT}:16020" # regionserver port, used by hbase region servers for handling read and write requests from clients.
      - "${HBASE_REGIONSERVER_UI_PORT}:16030" # regionserver ui port
    environment:
      - HBASE_ZOOKEEPER_QUORUM=zookeeper
      - HBASE_ZOOKEEPER_CLIENT_PORT=2181

 # Apache Ozone section

  datanode:
    container_name: ozone-datanode
    image: apache/ozone:1.4.0
    ports:
      - "${OZONE_DATANODE_PORT}:9864"
    command: ["ozone","datanode"]
    env_file:
      - ./ozone-config.env
    networks:
      - kafka-network
    volumes:
      - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  om:
    image: apache/ozone:1.4.0
    container_name: ozone-om
    ports:
      - "${OZONE_OM_UI_PORT}:9874"
    environment:
      ENSURE_OM_INITIALIZED: /data/metadata/om/current/VERSION
      WAITFOR: scm:9876
    env_file:
      - ./ozone-config.env
    command: ["ozone","om"]
    networks:
      - kafka-network
    volumes:
      - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  scm:
    image: apache/ozone:1.4.0
    container_name: ozone-scm
    ports:
      - "${OZONE_SCM_UI_PORT}:9876"
    env_file:
      - ./ozone-config.env
    environment:
      ENSURE_SCM_INITIALIZED: /data/metadata/scm/current/VERSION
    command: ["ozone","scm"]
    networks:
      - kafka-network
    volumes:
      - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  recon:
    image: apache/ozone:1.4.0
    container_name: ozone-recon
    ports:
      - "${OZONE_RECON_UI_PORT}:9888"
    env_file:
      - ./ozone-config.env
    command: ["ozone","recon"]
    networks:
      - kafka-network
    volumes:
      - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  s3g:
    image: apache/ozone:1.4.0
    container_name: ozone-s3g
    ports:
      - "${OZONE_S3G_S3_GATEWAY_ENDPOINT_PORT}:9878"
    env_file:
      - ./ozone-config.env
    command: ["ozone","s3g"]
    networks:
      - kafka-network
    volumes:
      - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml


  airflow_postgres:
    image: postgres:13
    container_name: airflow_db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_postgres_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - kafka-network



  redis:
    # Redis is limited to 7.2-bookworm due to licencing change
    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
    image: redis:7.2-bookworm
    container_name: airflow_redis
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always
    networks:
      - kafka-network


  airflow-webserver:
    <<: *airflow-common
    command: webserver
    container_name: airflow_webserver
    ports:
      - "${AIRFLOW_UI_PORT}:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    container_name: airflow_scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    container_name: airflow_worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    container_name: airflow_triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    container_name: airflow_init
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow
    container_name: airflow_cli

  # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/
  flower:
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    container_name: airflow_flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully


  # Apache Hadoop Section (HDFS deprecated in favor of Ozone, todo - switch YARN cluster to Ozone at some point if needed)

  # NOTE: Hadoop itself includes HDFS (Hadoop Distributed File System) as one of its core components. 
  # NOTE: When you install Hadoop, you automatically get HDFS along with it. 
  # NOTE: HDFS is the primary storage layer in the Hadoop ecosystem, providing distributed storage for large-scale data processing. 
  # NOTE: So, you don't need to separately install HDFS when setting up a Hadoop cluster; it comes bundled with the Hadoop distribution.

  # Manages the filesystem namespace and metadata for HDFS (Hadoop Distributed File System). HDFS Component
#  namenode:
#    image: apache/hadoop:3
#    container_name: hadoop-namenode
#    hostname: namenode
#    command: ["hdfs", "namenode"]
#    ports:
#      - "${HDFS_NAMENODE_UI_PORT}:9870"
#    env_file:
#      - ./hadoop-config.env
#    environment:
#      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"


  # Stores actual data blocks and serves read and write requests from clients in HDFS. HDFS Component
#  datanode:
#    image: apache/hadoop:3
#    container_name: hadoop-datanode
#    command: ["hdfs", "datanode"]
#    env_file:
#      - ./hadoop-config.env

  # Manages resources in the YARN (Yet Another Resource Negotiator) cluster, allocating resources to applications.
#  resourcemanager:
#    image: apache/hadoop:3
#    container_name: hadoop-resourcemanager
#    hostname: resourcemanager
#    command: ["yarn", "resourcemanager"]
#    ports:
#       - "${HADOOP_RESOURCEMANAGER_UI_PORT}:8088"
#    env_file:
#      - ./hadoop-config.env
#    volumes:
#      - ./test.sh:/opt/test.sh

  # Manages resources and executes tasks on individual nodes in the YARN cluster.
#  nodemanager:
#    image: apache/hadoop:3
#    container_name: hadoop-nodemanager
#    command: ["yarn", "nodemanager"]
#    env_file:
#      - ./hadoop-config.env

  # The holy grail of monitoring - with this we can have out real time graphs
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    networks:
      - kafka-network
    ports:
      - "${GRAFANA_UI_PORT}:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
    volumes:
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards

 # Machine Learning Tools

   # MLFlow Tracking Server to track experiments for ML
  tracking_server:
    restart: always
    build:
      context: .
      dockerfile: mlflowDockerfile
    container_name: mlflow_server
    depends_on:
      - mlflow_db
      - s3
    ports:
      - "${MLFLOW_PORT}:5000"
    networks:
      - kafka-network
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_ACCESS_KEY}
      - MLFLOW_S3_ENDPOINT_URL=http://s3:${MINIO_PORT}
      - MLFLOW_S3_IGNORE_TLS=true
    command: >
      mlflow server
      --backend-store-uri postgresql://${MLFLOW_PG_USER}:${MLFLOW_PG_PASSWORD}@mlflow_db:${MLFLOW_PG_PORT}/${MLFLOW_PG_DATABASE}
      --host 0.0.0.0
      --serve-artifacts
      --artifacts-destination s3://${MLFLOW_BUCKET_NAME}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MLFLOW_PORT}/"]
      interval: 30s
      timeout: 10s
      retries: 3

  mlflow_db:
    restart: always
    image: postgres
    container_name: mlflow_db
    expose:
      - "${MLFLOW_PG_PORT}"
    networks:
      - kafka-network
    environment:
      - POSTGRES_USER=${MLFLOW_PG_USER}
      - POSTGRES_PASSWORD=${MLFLOW_PG_PASSWORD}
      - POSTGRES_DATABASE=${MLFLOW_PG_DATABASE}
    volumes:
      - mlflow_db_data:/var/lib/postgresql/data/
    healthcheck:
      test: ["CMD", "pg_isready", "-p", "${MLFLOW_PG_PORT}", "-U", "${MLFLOW_PG_USER}"]
      interval: 5s
      timeout: 5s
      retries: 3

  # Now we add the pgadmin4 service so we can monitor db contents should we need to
  pgadmin4:
    image: dpage/pgadmin4:latest
    container_name: pgadmin4
    networks:
      - kafka-network
    environment:
      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_DEFAULT_EMAIL}
      - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_DEFAULT_PASSWORD}
    ports:
      - "${PGADMIN4_PORT}:80"
    volumes:
      - ./pgadmin4_psql_servers.json:/pgadmin4/servers.json

  # S3 storage for MLFlow
  s3:
    restart: always
    image: minio/minio:latest
    container_name: mlflow_minio
    volumes:
      - minio_data:/data
    ports:
      - "${MINIO_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:9001"
    networks:
      - kafka-network
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_ADDRESS=${MINIO_ADDRESS}
      - MINIO_PORT=${MINIO_PORT}
      - MINIO_STORAGE_USE_HTTPS=${MINIO_STORAGE_USE_HTTPS}
      - MINIO_CONSOLE_ADDRESS=${MINIO_CONSOLE_ADDRESS}
    command: server --console-address ":${MINIO_CONSOLE_PORT}" /data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  s3_mlflow_createbuckets:
    container_name: mlflow_create_bucket_job
    image: minio/mc
    depends_on:
      - s3
    networks:
      - kafka-network
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set s3minio http://s3:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      /usr/bin/mc mb s3minio/${MLFLOW_BUCKET_NAME};
      /usr/bin/mc policy set public s3minio/${MLFLOW_BUCKET_NAME};
      exit 0;
      "

  s3_mlflow_createaccesskey:
    container_name: mlflow_create_accesskey_job
    image: minio/mc
    depends_on:
      - s3
    networks:
      - kafka-network
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set s3minio http://s3:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      /usr/bin/mc admin user svcacct add --access-key "${MINIO_ACCESS_KEY}" --secret-key "${MINIO_SECRET_ACCESS_KEY}" --name mlflow --description connection s3minio ${MINIO_ROOT_USER};
      exit 0;
      "
volumes:
  mlflow_db_data:
  minio_data:
  spark-logs:
  hbase_data:
  airflow-logs:
  airflow-plugins:
  airflow_postgres_db_data:
